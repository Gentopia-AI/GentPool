{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Demo: Math Benchmark Selection\n",
    "Generally, a good ALM eval task is something hard for vanilla LLMs, where we hope tools come in to assist.\n",
    "\n",
    "Here we demo how to retrive some math eval from MATH dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:04.038012037Z",
     "start_time": "2023-06-29T07:06:02.715352212Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from gentopia import AgentAssembler\n",
    "from bench.grader import GateGrader, BatchGateGrader\n",
    "from gentopia.llm import OpenAIGPTClient\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:04.040710857Z",
     "start_time": "2023-06-29T07:06:04.039689751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Recursive function to load json files from a path and its subdirectories\n",
    "def load_from_path_recursive(path):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(root, file), 'r') as f:\n",
    "                    data.append(json.load(f))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:04.196744495Z",
     "start_time": "2023-06-29T07:06:04.041193717Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read MATH dataset\n",
    "data = load_from_path_recursive(\"../benchmark/raw/MATH/\")\n",
    "# Initial filter by level of difficulty \n",
    "hard_data = []\n",
    "for data in data:\n",
    "    if data[\"level\"] in [\"Level 5\"]:\n",
    "        hard_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:04.199697353Z",
     "start_time": "2023-06-29T07:06:04.197984405Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hard_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:04.242159821Z",
     "start_time": "2023-06-29T07:06:04.242007362Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use vanilla gpt-3.5-turbo as threshold\n",
    "dummy_agent = AgentAssembler(file=\"../eval/config/chatgpt.yaml\").get_agent()\n",
    "\n",
    "eval_llm = OpenAIGPTClient(model_name=\"gpt-4\")\n",
    "grader = GateGrader(llm=eval_llm)\n",
    "batch_grader = BatchGateGrader(llm=eval_llm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Single Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:04.350524380Z",
     "start_time": "2023-06-29T07:06:04.303288595Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem': 'Find the largest negative integer $x$ which satisfies the congruence $34x+6\\\\equiv 2\\\\pmod {20}$.',\n",
       " 'level': 'Level 5',\n",
       " 'type': 'Number Theory',\n",
       " 'solution': 'We can simplify the congruence as follows (all of the following congruences are equivalent):\\n\\\\begin{align*}\\n34x+6&\\\\equiv 2\\\\pmod {20}\\\\\\\\\\n14x+6&\\\\equiv 2\\\\pmod {20}\\\\\\\\\\n14x&\\\\equiv 16\\\\pmod {20}\\\\\\\\\\n7x&\\\\equiv 8\\\\pmod {10}\\\\\\\\\\n21x&\\\\equiv 8\\\\cdot 3\\\\pmod {10}\\\\\\\\\\nx&\\\\equiv 24\\\\pmod{10}\\\\\\\\\\nx&\\\\equiv 4\\\\pmod{10}\\\\\\\\\\nx&\\\\equiv \\\\boxed{-6}\\\\pmod{10}.\\n\\\\end{align*}'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:11.732089292Z",
     "start_time": "2023-06-29T07:06:05.002868714Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We can simplify the congruence as follows: \\\\begin{align*}\\n34x+6&\\\\equiv 2\\\\pmod{20} \\\\\\\\\\n34x&\\\\equiv -4\\\\pmod{20} \\\\\\\\\\n17x&\\\\equiv -2\\\\pmod{10} \\\\\\\\\\n17x&\\\\equiv 8\\\\pmod{10} \\\\\\\\\\n7x&\\\\equiv 8\\\\pmod{10}.\\n\\\\end{align*}We can then find the inverse of $7$ modulo $10$ by testing values: \\\\begin{align*}\\n7\\\\cdot 1&\\\\equiv 7\\\\pmod{10} \\\\\\\\\\n7\\\\cdot 2&\\\\equiv 4\\\\pmod{10} \\\\\\\\\\n7\\\\cdot 3&\\\\equiv 1\\\\pmod{10}.\\n\\\\end{align*}Therefore, the inverse of $7$ modulo $10$ is $3$. Multiplying both sides of $7x\\\\equiv 8\\\\pmod{10}$ by $3$, we get \\\\[21x\\\\equiv 24\\\\pmod{10}.\\\\]Since $21x\\\\equiv 1x\\\\pmod{10}$, we have $x\\\\equiv 24\\\\pmod{10}$. The largest negative integer $x$ satisfying this congruence is $\\\\boxed{-6}$.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = dummy_agent.run(hard_data[0][\"problem\"]).output\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:27.378658650Z",
     "start_time": "2023-06-29T07:06:26.410502076Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentOutput(output='passed', cost=0.01704, token_usage=567)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.run(hard_data[0][\"problem\"], hard_data[0][\"solution\"], pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Batch Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T07:06:54.558468852Z",
     "start_time": "2023-06-29T07:06:54.515324934Z"
    }
   },
   "outputs": [],
   "source": [
    "problems = [\"Answer in short: \" + data[\"problem\"] for data in hard_data]\n",
    "solutions = [data[\"solution\"] for data in hard_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-29T08:39:17.254768599Z",
     "start_time": "2023-06-29T07:06:59.425198906Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [15:16<39:32, 33.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [37:32<13:51, 32.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: The server is overloaded or not ready yet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [51:23<00:00, 30.83s/it]\n"
     ]
    }
   ],
   "source": [
    "BS = 3\n",
    "SAMPLE = 300\n",
    "\n",
    "probs, sols, preds, grades = [], [], [], []\n",
    "cost, tokens = 0, 0\n",
    "for i in tqdm(range(0, SAMPLE, BS)):\n",
    "    batch_problems = problems[i:i+BS]\n",
    "    batch_solutions = solutions[i:i+BS]\n",
    "    try:\n",
    "        batch_preds = [dummy_agent.run(prob).output for prob in batch_problems]\n",
    "        res = batch_grader.run(batch_problems, batch_solutions, batch_preds)\n",
    "        probs += batch_problems\n",
    "        sols += batch_solutions\n",
    "        preds += batch_preds\n",
    "        grades += res.output.split(\",\")\n",
    "        cost += res.cost\n",
    "        tokens += res.token_usage\n",
    "    except Exception as e:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294, 294, 294, 294)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(probs), len(sols), len(preds), len(grades)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select failed tasks and save to bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New tasks saved to bench: 257\n"
     ]
    }
   ],
   "source": [
    "failed_index = [i for i, g in enumerate(grades) if g == \"failed\"]\n",
    "probs_failed = [probs[i] for i in failed_index]\n",
    "sols_failed = [sols[i] for i in failed_index]\n",
    "\n",
    "#Save into separate json files\n",
    "for i, (p, s) in enumerate(zip(probs_failed, sols_failed)):\n",
    "    with open(f\"../benchmark/math/math_{i}.json\", 'w') as f:\n",
    "        json.dump({\"problem\": p, \"solution\": s}, f, indent=4)\n",
    "\n",
    "print(f\"New tasks saved to bench: {len(probs_failed)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total eval cost: $5.634600000000001, Token usage: 186448\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total eval cost: ${cost}, Token usage: {tokens}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Careful!\n",
    "\n",
    "We have not verify the correctness of these MATH problems themselves.\n",
    "\n",
    "The failure could be caused by the error in dataset \"ground truth\" themselves. (Especially for older NLP benchmarks)\n",
    "\n",
    "Need to manually verify the correctness before using as bench eval set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

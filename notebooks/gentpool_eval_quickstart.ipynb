{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GentPool: Eval Quickstart\n",
    "### This notebook aims to give a quick overview of GentPool Eval methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T05:04:07.290482240Z",
     "start_time": "2023-07-13T05:04:05.993950789Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gentopia import AgentAssembler\n",
    "from gentpool.bench.grader import GateGrader\n",
    "from gentpool.bench.eval import EvalPipeline\n",
    "import json\n",
    "\n",
    "# Load API Keys\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single eval with agent, grader and task.\n",
    "\n",
    "The basic usage of GentPool/bench components.\n",
    "\n",
    "Use *AgentAssembler* to assemble an agent, instantiate a *Grader* and provide grader inputs to get an evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T05:04:09.217322273Z",
     "start_time": "2023-07-13T05:04:09.211885160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'GateArgsSchema', 'type': 'object', 'properties': {'task': {'title': 'Task', 'type': 'string'}, 'ground_truth': {'title': 'Ground Truth', 'type': 'string'}, 'prediction': {'title': 'Prediction', 'type': 'string'}}, 'required': ['task', 'ground_truth', 'prediction']}\n"
     ]
    }
   ],
   "source": [
    "# Assemble agent from config.\n",
    "agent = AgentAssembler(file=\"../gentpool/pool/mathria/agent.yaml\").get_agent()\n",
    "# Instantiate a Grader\n",
    "grader = GateGrader()\n",
    "print(grader.args_schema.schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T05:04:23.305110067Z",
     "start_time": "2023-07-13T05:04:09.517810864Z"
    }
   },
   "outputs": [],
   "source": [
    "math = json.load(open(\"../benchmark/public/reasoning/math/math_40.json\"))\n",
    "task = math['problem']\n",
    "ground_truth = math['solution']\n",
    "prediction = agent.run(task).output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T05:04:23.921538779Z",
     "start_time": "2023-07-13T05:04:23.306332027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentOutput(output='failed', cost=0.025349999999999998, token_usage=844)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get grades (GatedGrader outputs \"passed\" or \"failed\")\n",
    "grader.run(task=task, ground_truth=ground_truth, prediciton=prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config and EvalPipeline\n",
    "\n",
    "For comprehensive eval over GentPool benchmark, use a config file to invoke *EvalPipeline* and receive an eval report. See GentPool/eval_config.yaml for an example.\n",
    "\n",
    "**Warning**: Cost can be non-trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-13T05:06:07.935145290Z",
     "start_time": "2023-07-13T05:04:23.920782711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> EVALUATING: knowledge/world_knowledge ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: knowledge/domain_specific_knowledge ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: knowledge/web_retrieval ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: reasoning/math ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: reasoning/coding ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: reasoning/planning ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: reasoning/commonsense ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: safety/integrity ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: safety/harmless ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: multilingual/translation ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: multilingual/understanding ...\n",
      ">>> Running Eval 1/1 ...\n",
      "> EVALUATING: robustness/consistency ...\n",
      "> EVALUATING: robustness/resilience ...\n",
      "\n",
      "### FINISHING Agent EVAL PIPELINE ### \n",
      " (づ￣ ³￣)づ \n",
      "--------------Task Specific-------------- \n",
      "Score of knowledge/world_knowledge: 0.0 \n",
      "Score of knowledge/domain_specific_knowledge: 0.0 \n",
      "Score of knowledge/web_retrieval: 0.0 \n",
      "Score of reasoning/math: 0.0 \n",
      "Score of reasoning/coding: 0.0 \n",
      "Score of reasoning/planning: 100.0 \n",
      "Score of reasoning/commonsense: 0.0 \n",
      "Score of safety/integrity: 100.0 \n",
      "Score of safety/harmless: 0.0 \n",
      "Score of multilingual/translation: 0.0 \n",
      "Score of multilingual/understanding: 0.0 \n",
      "Score of robustness/consistency: 0.0 \n",
      "Score of robustness/resilience: 0.0 \n",
      "-----------Overal (Weighted Avg)----------- \n",
      "Agent score: 18.181818181818183 \n",
      "Agent run exception rate: 0.0% \n",
      "Avg runtime per task: 7.46s \n",
      "Avg cost per 1000 runs: $1.472 \n",
      "Avg token usage per task: 922.5 tokens \n",
      "... And the total cost for evaluation $0.1128 \n",
      "Your agent needs some additional tuning (╯°□°）╯︵ ┻━┻)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EvalPipelineResult(eval_results={'knowledge/world_knowledge': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=6.651751518249512, avg_cost=0.0018174999999999999, avg_token_usage=1128.0, eval_cost=0.013739999999999999), 'knowledge/domain_specific_knowledge': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=5.33078145980835, avg_cost=0.001386, avg_token_usage=893.0, eval_cost=0.011459999999999998), 'knowledge/web_retrieval': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=10.239863634109497, avg_cost=0.001128, avg_token_usage=687.0, eval_cost=0.00789), 'reasoning/math': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=19.004570960998535, avg_cost=0.001885, avg_token_usage=1121.0, eval_cost=0.01923), 'reasoning/coding': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=6.613209962844849, avg_cost=0.002029, avg_token_usage=1272.0, eval_cost=0.0), 'reasoning/planning': EvalResult(score=1.0, fail_rate=0.0, avg_runtime=8.17550253868103, avg_cost=0.0028815, avg_token_usage=1821.0, eval_cost=0.02445), 'reasoning/commonsense': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=10.320070028305054, avg_cost=0.0013125, avg_token_usage=826.0, eval_cost=0.011579999999999998), 'safety/integrity': EvalResult(score=1.0, fail_rate=0.0, avg_runtime=2.02425479888916, avg_cost=0.0010275, avg_token_usage=665.0, eval_cost=0.00366), 'safety/harmless': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=10.136386394500732, avg_cost=0.0013125, avg_token_usage=826.0, eval_cost=0.011579999999999998), 'multilingual/translation': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=1.7238433361053467, avg_cost=0.0006825, avg_token_usage=439.0, eval_cost=0.0045), 'multilingual/understanding': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=1.8425641059875488, avg_cost=0.0007315, avg_token_usage=469.0, eval_cost=0.00471), 'robustness/consistency': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=0.0, avg_cost=0.0, avg_token_usage=0.0, eval_cost=0.0), 'robustness/resilience': EvalResult(score=0.0, fail_rate=0.0, avg_runtime=0.0, avg_cost=0.0, avg_token_usage=0.0, eval_cost=0.0)}, avg_score=0.18181818181818182, avg_fail_rate=0.0, avg_runtime=7.460254430770872, avg_cost=0.0014721363636363636, avg_token_usage=922.4545454545455, total_eval_cost=0.11279999999999998)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = EvalPipeline(eval_config=\"../config/eval_config.yaml\")\n",
    "eval.run_eval(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
